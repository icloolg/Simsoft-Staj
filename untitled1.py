# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eAfAd7MudsVfBhpYrFijh_RrlDAuisUz
"""

!pip install ultralytics

import cv2

import math

import cv2
from ultralytics import YOLO
import math

# CV2 but prettier and easier to use
import cvzone

# Importing all functions from SORT
from sort import *

# cap = cv2.VideoCapture(0) #for webcam
# cap.set(3,1280)
# cap.set(4,720)

cap = cv2.VideoCapture("/content/football.mp4")
model =  YOLO("yolos/yolov8n.pt")

!pip install cvzone

!pip install sorted

!yolo track source="/content/football.mp4" save=True



!pip install google-colab-patches
from collections import defaultdict
import cv2
from ultralytics import YOLO
from ultralytics.utils.plotting import Annotator, colors
from google.colab.patches import cv2_imshow

# Dictionary to store tracking history with default empty lists
track_history = defaultdict(lambda: [])

# Load the YOLO model with segmentation capabilities
model = YOLO("yolov8n-seg.pt")

# Open the video file
cap = cv2.VideoCapture("/content/football.mp4")

# Retrieve video properties: width, height, and frames per second
w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))

# Initialize video writer to save the output video with the specified properties
out = cv2.VideoWriter("instance-segmentation-object-tracking.avi", cv2.VideoWriter_fourcc(*"MJPG"), fps, (w, h))

while True:
    # Read a frame from the video
    ret, im0 = cap.read()
    if not ret:
        print("Video frame is empty or video processing has been successfully completed.")
        break

    # Create an annotator object to draw on the frame
    annotator = Annotator(im0, line_width=2)

    # Perform object tracking on the current frame
    results = model.track(im0, persist=True)

    # Check if tracking IDs and masks are present in the results
    if results[0].boxes.id is not None and results[0].masks is not None:
        # Extract masks, tracking IDs, bounding boxes, class names, and confidence scores
        masks = results[0].masks.xy
        track_ids = results[0].boxes.id.int().cpu().tolist()
        boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)
        class_ids = results[0].boxes.cls.int().cpu().tolist()  # Get class IDs
        confidences = results[0].boxes.conf.cpu().numpy()      # Get confidence scores

        # Retrieve class names from the model (if you have them stored)
        class_names = model.names  # Class names dictionary

        # Annotate each mask with its corresponding tracking ID, class name, confidence score, and color
        for mask, track_id, box, class_id, conf in zip(masks, track_ids, boxes, class_ids, confidences):
            annotator.seg_bbox(mask=mask, mask_color=colors(track_id, True))
            x1, y1, x2, y2 = box

            # Get the class name using the class ID
            class_name = class_names[class_id]

            # Put text on the frame with simpler and smaller font, each on a new line
            cv2.putText(im0, f"ID: {track_id}", (x1, y1 - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            cv2.putText(im0, f"Class: {class_name}", (x1, y1 - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            cv2.putText(im0, f"Conf: {conf:.2f}", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

    # Write the annotated frame to the output video
    out.write(im0)

    # Display the annotated frame using cv2_imshow
    cv2_imshow(im0)

    # Exit the loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

# Release the video writer and capture objects, and close all OpenCV windows
out.release()
cap.release()
cv2.destroyAllWindows()